{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código gera o gráfico da margem de erro, onde o experimento foi executado por 100 vezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "sns.set_theme(style='darkgrid')\n",
    "tamanho_fonte = 25\n",
    "lista = []\n",
    "try:\n",
    "    niid_iid = ['IID', 'NIID']\n",
    "    ataques = ['ALTERNA_INICIO', 'ATACANTES', 'EMBARALHA', 'INVERTE_TREINANDO', 'INVERTE_SEM_TREINAR', 'INVERTE_CONVEGENCIA', 'ZEROS', 'RUIDO_GAUSSIANO', 'NORMAL']\n",
    "    data_set = ['MNIST', 'CIFAR10']\n",
    "    modelos = ['DNN', 'CNN']\n",
    "\n",
    "    for i, j, k, l in product(niid_iid, ataques, data_set, modelos):\n",
    "        file_list = glob.glob(f'TESTES/rodou_dez_vezes/{i}/LOG_EVALUATE/{j}_{k}_{l}*.csv')\n",
    "        lista.append(file_list)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao processar: {str(e)}\")\n",
    "\n",
    "for caminhos_arquivos in lista:\n",
    "    rotulos = []\n",
    "    for arquivo in caminhos_arquivos:\n",
    "        try:\n",
    "            arquivo = arquivo.replace('\\\\', '/')\n",
    "\n",
    "            extensao = arquivo.split('.')\n",
    "            caminho = extensao[0].split('/')\n",
    "            base = caminho[4].split('_')\n",
    "            rotulo = f'{base[-1]}'\n",
    "            rotulos.append(rotulo)\n",
    "\n",
    "            plt.figure(figsize=(9, 5))\n",
    "            for i, arquivo_atual in enumerate(caminhos_arquivos):\n",
    "                data = pd.read_csv(arquivo_atual, header=None)\n",
    "                data.columns = ['server_round', 'cid', 'accuracy', 'loss']\n",
    "\n",
    "                sns.lineplot(x='server_round', y='accuracy', data=data, label=f'{rotulos[i]}')\n",
    "\n",
    "            xticks = np.arange(0,101,10)\n",
    "            plt.xticks(xticks, fontsize=tamanho_fonte)\n",
    "            plt.xticks(fontsize=tamanho_fonte)\n",
    "            plt.yticks(fontsize=tamanho_fonte)\n",
    "\n",
    "            plt.grid(color='k', linestyle='--', linewidth=0.5, axis='both', alpha=0.1)\n",
    "            plt.legend(\n",
    "                loc='best',\n",
    "                fontsize=tamanho_fonte,\n",
    "                ncol=1,\n",
    "                title='# Round',\n",
    "                title_fontsize=tamanho_fonte\n",
    "            )\n",
    "            plt.savefig(f'TESTES/{caminho[1]}/GRAFICOS/{base[0]}_{base[1]}_{base[2]}_accuracy.png', dpi=300)\n",
    "            plt.close('all')\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(9, 5))\n",
    "            for i, arquivo_atual in enumerate(caminhos_arquivos):\n",
    "                data = pd.read_csv(arquivo_atual, header=None)\n",
    "                data.columns = ['server_round', 'cid', 'accuracy', 'loss']\n",
    "\n",
    "                sns.lineplot(x='server_round', y='loss', data=data, label=f'{rotulos[i]}')\n",
    "\n",
    "            xticks = np.arange(0,101,10)\n",
    "            plt.xticks(xticks, fontsize=tamanho_fonte)\n",
    "            plt.xticks(fontsize=tamanho_fonte)\n",
    "            plt.yticks(fontsize=tamanho_fonte)\n",
    "\n",
    "            plt.grid(color='k', linestyle='--', linewidth=0.5, axis='both', alpha=0.1)\n",
    "            plt.legend(\n",
    "                loc='best',\n",
    "                fontsize=tamanho_fonte,\n",
    "                ncol=1,\n",
    "                title='# Round',\n",
    "                title_fontsize=tamanho_fonte\n",
    "            )\n",
    "            plt.savefig(f'TESTES/{caminho[1]}/GRAFICOS/{base[0]}_{base[1]}_{base[2]}_loss.png', dpi=300)\n",
    "            plt.close('all')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ocorreu um erro ao processar o arquivo {arquivo}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código abaixo gera a tabela comparativa entre todas os ataques executados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calcular_media(lista):\n",
    "    return round(sum(lista) / len(lista), 2)\n",
    "\n",
    "def criar_tabela(media_geral_acuracia, titulo, caminho_destino):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "\n",
    "    table_data = [['Modelo', 'Média de Acurácia']]\n",
    "    for modelo, media_acuracia in media_geral_acuracia.items():\n",
    "        table_data.append([modelo, media_acuracia])\n",
    "\n",
    "    # Verificar se a lista table_data está vazia antes de tentar criar a tabela\n",
    "    if len(table_data) > 1:\n",
    "        table = ax.table(cellText=table_data, loc='center', cellLoc='center', colLabels=table_data.pop(0))\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(14)\n",
    "        table.scale(1.2, 1.2)\n",
    "\n",
    "        plt.title(titulo)\n",
    "        \n",
    "        # Verificar se o diretório de destino existe, se não, criá-lo\n",
    "        if not os.path.exists(caminho_destino):\n",
    "            os.makedirs(caminho_destino)\n",
    "        \n",
    "        plt.savefig(f'{caminho_destino}/{titulo}_accuracy.png', dpi=300)\n",
    "    else:\n",
    "        print(\"Nenhum dado encontrado para criar a tabela.\")\n",
    "\n",
    "tamanho_fonte = 25\n",
    "lista_iid_dnn = []\n",
    "lista_iid_cnn = []\n",
    "lista_niid_dnn = []\n",
    "lista_niid_cnn = []\n",
    "\n",
    "try:\n",
    "    niid_iid = ['IID', 'NIID']\n",
    "    ataques = ['ALTERNA_INICIO', 'ATACANTES', 'EMBARALHA', 'INVERTE_TREINANDO', 'INVERTE_SEM_TREINAR', 'INVERTE_CONVEGENCIA', 'ZEROS', 'RUIDO_GAUSSIANO', 'NORMAL']\n",
    "    data_set = ['MNIST', 'CIFAR10']\n",
    "    modelos = ['DNN', 'CNN']\n",
    "\n",
    "    for i, j, k, l in product(niid_iid, ataques, data_set, modelos):\n",
    "        file_list = glob.glob(f'TESTES/{i}/LOG_EVALUATE/{j}_{k}_{l}_*.csv')\n",
    "        if i == 'IID' and  l == 'DNN':        \n",
    "            lista_iid_dnn.append((j, l, file_list))\n",
    "        elif i == 'IID' and l == 'CNN':\n",
    "            lista_iid_cnn.append((j, l, file_list))        \n",
    "        elif i == 'NIID' and l == 'DNN':\n",
    "            lista_niid_dnn.append((j, l, file_list))\n",
    "        elif i == 'NIID' and l == 'CNN':\n",
    "            lista_niid_cnn.append((j, l, file_list))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao processar: {str(e)}\")\n",
    "\n",
    "# Dicionário para armazenar médias de acurácia de cada modelo\n",
    "media_acuracia_por_modelo_iid_dnn = {}\n",
    "media_acuracia_por_modelo_iid_cnn = {}\n",
    "media_acuracia_por_modelo_niid_dnn = {}\n",
    "media_acuracia_por_modelo_niid_cnn = {}\n",
    "\n",
    "# Processar os dados e calcular média de acurácia para cada modelo de ataque\n",
    "\n",
    "# Processar os dados e calcular média de acurácia para cada modelo de ataque\n",
    "def processar_arquivos(lista, media_acuracia_por_modelo):\n",
    "    for ataque, modelo, arquivos in lista:\n",
    "        chave = f'{ataque}_{modelo}'  # Usar uma chave única que leve em consideração o ataque e o modelo\n",
    "        if chave not in media_acuracia_por_modelo:\n",
    "            media_acuracia_por_modelo[chave] = []\n",
    "        for arquivo in arquivos:\n",
    "            try:\n",
    "                arquivo = arquivo.replace('\\\\', '/')\n",
    "                extensao = arquivo.split('.')\n",
    "                caminho = '.'.join(extensao[:-1]).split('/')\n",
    "                base = caminho[-1].split('_')\n",
    "                rotulo = f'{base[-1]}'\n",
    "\n",
    "                data = pd.read_csv(arquivo, header=None)\n",
    "                data.columns = ['server_round', 'cid', 'accuracy', 'loss']\n",
    "                media_round = calcular_media(data['accuracy'])\n",
    "                \n",
    "                media_acuracia_por_modelo[chave].append(media_round)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Ocorreu um erro ao processar o arquivo {arquivo}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Processar os arquivos IID para DNN\n",
    "processar_arquivos(lista_iid_dnn, media_acuracia_por_modelo_iid_dnn)\n",
    "\n",
    "# Processar os arquivos NIID para DNN\n",
    "processar_arquivos(lista_niid_dnn, media_acuracia_por_modelo_niid_dnn)\n",
    "\n",
    "# Processar os arquivos IID para CNN\n",
    "processar_arquivos(lista_iid_cnn, media_acuracia_por_modelo_iid_cnn)\n",
    "\n",
    "# Processar os arquivos NIID para CNN\n",
    "processar_arquivos(lista_niid_cnn, media_acuracia_por_modelo_niid_cnn)\n",
    "\n",
    "# Calcular a média geral de acurácia para cada modelo e tipo\n",
    "media_geral_acuracia_iid_dnn = {ataque: calcular_media(media_acuracias) for ataque, media_acuracias in media_acuracia_por_modelo_iid_dnn.items()}\n",
    "media_geral_acuracia_niid_dnn = {ataque: calcular_media(media_acuracias) for ataque, media_acuracias in media_acuracia_por_modelo_niid_dnn.items()}\n",
    "media_geral_acuracia_iid_cnn = {ataque: calcular_media(media_acuracias) for ataque, media_acuracias in media_acuracia_por_modelo_iid_cnn.items()}\n",
    "media_geral_acuracia_niid_cnn = {ataque: calcular_media(media_acuracias) for ataque, media_acuracias in media_acuracia_por_modelo_niid_cnn.items()}\n",
    "\n",
    "# Criar tabelas individuais para cada combinação de tipo (IID/NIID) e modelo (DNN/CNN)\n",
    "criar_tabela(media_geral_acuracia_iid_dnn, 'IID - DNN', 'TESTES/IID/GRAFICOS')\n",
    "criar_tabela(media_geral_acuracia_niid_dnn, 'NIID - DNN', 'TESTES/NIID/GRAFICOS')\n",
    "criar_tabela(media_geral_acuracia_iid_cnn, 'IID - CNN', 'TESTES/IID/GRAFICOS')\n",
    "criar_tabela(media_geral_acuracia_niid_cnn, 'NIID - CNN', 'TESTES/NIID/GRAFICOS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gera a matriz confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "try:\n",
    "    modelos = ['DNN', 'CNN']\n",
    "    niid_iid = ['IID','NIID']        \n",
    "    ataques = ['ALTERNA_INICIO', 'ATACANTES', 'EMBARALHA', 'INVERTE_TREINANDO', 'INVERTE_SEM_TREINAR', 'INVERTE_CONVEGENCIA', 'ZEROS', 'RUIDO_GAUSSIANO', 'NORMAL']\n",
    "    data_set = ['MNIST', 'CIFAR10']                        \n",
    "    alpha_dirichlet = [0.0]\n",
    "    noise_gaussiano = [0.0, 0.1]\n",
    "    round_inicio = [4,8]\n",
    "    per_cents_atacantes = [40]\n",
    "\n",
    "    lista = set()\n",
    "    combinacoes_unicas = set()        \n",
    "\n",
    "    for i, j, k, l, m, n, o, p in product(niid_iid, ataques, data_set, modelos, per_cents_atacantes, alpha_dirichlet, noise_gaussiano, round_inicio):                    \n",
    "        file_list = glob.glob(f'TESTES/{i}/LOG_ACERTOS/*.csv') \n",
    "        combinacao = (i, j, k, l, m, n, o, p)  \n",
    "           \n",
    "        if i == 'IID' and n > 0: \n",
    "                   \n",
    "            continue\n",
    "             \n",
    "        if j != 'RUIDO_GAUSSIANO' and o > 0:        \n",
    "            continue\n",
    "\n",
    "        if (k == 'MNIST' and l == 'CNN') or (k == 'CIFAR10' and l == 'DNN'):            \n",
    "            continue\n",
    "        \n",
    "        if combinacao not in combinacoes_unicas:                            \n",
    "            combinacoes_unicas.add(combinacao)        \n",
    "            lista.update(file_list)\n",
    "        else:\n",
    "                continue\n",
    "             \n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao processar: {str(e)}\")\n",
    "\n",
    "for arquivo in lista:\n",
    "    try:\n",
    "        plt.figure()  # Criar uma nova figura para cada iteração do loop\n",
    "        \n",
    "        arquivo = arquivo.replace('\\\\', '/')\n",
    "        extensao = arquivo.split('.')\n",
    "        caminho = '.'.join(extensao[:-1]).split('/')        \n",
    "        base = caminho[-1].split('_')\n",
    "       \n",
    "        data = pd.read_csv(arquivo, header=None)        \n",
    "\n",
    "        situacao = data[2].astype(int)\n",
    "        prev = data[3].apply(lambda x: int(x.strip('[]')) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(situacao, prev)\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Matriz de Confusão')\n",
    "        plt.colorbar()\n",
    "        classes = ['Negativo', 'Positivo']  \n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.ylabel('Classe Real')\n",
    "        plt.xlabel('Classe Prevista')\n",
    "        plt.tight_layout()\n",
    "        arquivo_ = f'TESTES/{caminho[1]}/LOG_ACERTOS/GRAFICOS/{base[0]}_{base[1]}_{base[2]}_{base[3]}_{base[4]}_{base[5]}_{base[6]}_{base[7]}_MATRIZ_CONFUSAO.png'\n",
    "        os.makedirs(os.path.dirname(arquivo_), exist_ok=True)\n",
    "        plt.savefig(arquivo_, dpi=300)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao processar: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o código abaixo é responsável por gerar as tabelas de acurácia de todos os experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calcular_media(arquivos):\n",
    "    return round(sum(arquivos) / len(arquivos), 2)\n",
    "\n",
    "tamanho_fonte = 25\n",
    "lista = []\n",
    "combinacoes_unicas = []\n",
    "\n",
    "try:\n",
    "    modelos = ['DNN', 'CNN']\n",
    "    niid_iid = ['IID','NIID']        \n",
    "    ataques = ['ALTERNA_INICIO', 'ATACANTES', 'EMBARALHA', 'INVERTE_TREINANDO', 'INVERTE_SEM_TREINAR', 'INVERTE_CONVEGENCIA', 'ZEROS', 'RUIDO_GAUSSIANO', 'NORMAL']\n",
    "    data_set = ['MNIST', 'CIFAR10']                        \n",
    "    alpha_dirichlet = [0.0,0.5]\n",
    "    noise_gaussiano = [0.0,0.1,]\n",
    "    round_inicio = [4]\n",
    "    per_cents_atacantes = [40]\n",
    "            \n",
    "   \n",
    "         \n",
    "\n",
    "    for i, j, k, l, m, n, o, p in product(niid_iid, ataques, data_set, modelos, per_cents_atacantes, alpha_dirichlet, noise_gaussiano, round_inicio):                    \n",
    "        file_list = glob.glob(f'TESTES/{i}/LOG_EVALUATE/{j}_{k}_{l}_{m}_{n}_{o}*.csv') \n",
    "        combinacao = (i, j, k, l, m, n, o, p)  \n",
    "            \n",
    "        if i == 'IID' and n > 0:           \n",
    "            continue\n",
    "\n",
    "        if j != 'RUIDO_GAUSSIANO' and o > 0:        \n",
    "            continue\n",
    "\n",
    "        if (k == 'MNIST' and l == 'CNN') or (k == 'CIFAR10' and l == 'DNN'):            \n",
    "            continue\n",
    "\n",
    "        if combinacao not in combinacoes_unicas:                  \n",
    "            combinacoes_unicas.append(combinacao)        \n",
    "            lista.append(file_list)\n",
    "        else:\n",
    "                continue\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao processar: {str(e)}\")\n",
    "\n",
    "for arquivos in lista:\n",
    "    rotulos = [] \n",
    "    for arquivo in arquivos:     \n",
    "        try:\n",
    "            arquivo = arquivo.replace('\\\\', '/')\n",
    "            extensao = arquivo.split('.')\n",
    "            caminho = '.'.join(extensao[:-1]).split('/') \n",
    "            nome_arquivo = caminho[3][:-1]             \n",
    "            base = caminho[-1].split('_')\n",
    "            rotulos.append(base[-1])\n",
    "            \n",
    "            # plt.figure(figsize=(9, 5))\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            for i, arquivo_atual in enumerate(arquivos):\n",
    "                data = pd.read_csv(arquivo_atual, header=None)\n",
    "                data.columns = ['server_round', 'cid', 'accuracy', 'loss']\n",
    "                media_round = data.groupby('server_round').agg({\n",
    "                    'accuracy': calcular_media,\n",
    "                }).reset_index()\n",
    "                \n",
    "                \n",
    "                plt.plot(media_round['server_round'], media_round['accuracy'], label=f'{rotulos[i]}', linewidth=3)\n",
    "                \n",
    "                \n",
    "            xticks = np.arange(0,21,2)\n",
    "            plt.xticks(xticks, fontsize=tamanho_fonte)\n",
    "            plt.xticks(fontsize=tamanho_fonte)\n",
    "            plt.yticks(fontsize=tamanho_fonte)\n",
    "            plt.ylabel('Accuracy', fontsize=tamanho_fonte)\n",
    "            plt.xlabel('Round', fontsize=tamanho_fonte)\n",
    "            # plt.xlabel('# Round', fontsize=tamanho_fonte, labelpad=20)\n",
    "            # plt.ylabel('Loss', fontsize=tamanho_fonte, labelpad=10)\n",
    "            plt.grid(color='k', linestyle='--', linewidth=0.5, axis='both', alpha=0.1)\n",
    "            # plt.legend(\n",
    "            #     loc='best',\n",
    "            #     fontsize=tamanho_fonte,\n",
    "            #     ncol=1,\n",
    "            #     title='# Round',\n",
    "            #     title_fontsize=tamanho_fonte\n",
    "            # )\n",
    "            arquivo_accuracy = f'TESTES/{caminho[1]}/GRAFICOS/{nome_arquivo}_accuracy.png'\n",
    "            os.makedirs(os.path.dirname(arquivo_accuracy), exist_ok=True)\n",
    "            plt.savefig(arquivo_accuracy, dpi=100)\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            for i, arquivo in enumerate(arquivos):\n",
    "                data = pd.read_csv(arquivo, header=None)\n",
    "                data.columns = ['server_round', 'cid', 'accuracy', 'loss']\n",
    "                media_round = data.groupby('server_round').agg({\n",
    "                    'loss': calcular_media,\n",
    "                }).reset_index()\n",
    "                \n",
    "                plt.plot(media_round['server_round'], media_round['loss'], label=f'{rotulos[i]}', linewidth=3)\n",
    "\n",
    "            xticks = np.arange(0,21,2)\n",
    "            plt.xticks(xticks, fontsize=tamanho_fonte)\n",
    "            plt.xticks(fontsize=tamanho_fonte)\n",
    "            plt.yticks(fontsize=tamanho_fonte)\n",
    "            plt.ylabel('Loss', fontsize=tamanho_fonte)\n",
    "            plt.xlabel('Round', fontsize=tamanho_fonte)\n",
    "            # plt.xlabel('# Round', fontsize=tamanho_fonte, labelpad=20)\n",
    "            # plt.ylabel('Loss', fontsize=tamanho_fonte, labelpad=10)\n",
    "            plt.grid(color='k', linestyle='--', linewidth=0.5, axis='both', alpha=0.1)\n",
    "            # plt.legend(\n",
    "            #     loc='best',\n",
    "            #     fontsize=tamanho_fonte,\n",
    "            #     ncol=1,\n",
    "            #     title='# Round',\n",
    "            #     title_fontsize=tamanho_fonte\n",
    "            # )\n",
    "            arquivo_loss = f'TESTES/{caminho[1]}/GRAFICOS/{nome_arquivo}_loss.png'\n",
    "            os.makedirs(os.path.dirname(arquivo_loss), exist_ok=True)\n",
    "            plt.savefig(arquivo_loss, dpi=100)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Ocorreu um erro ao processar o arquivo {arquivo}: {str(e)}\")\n",
    "            a=e\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
